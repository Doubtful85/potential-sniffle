{"id":"google/cloud/vision/v1/datatypes","name":"DataTypes","title":["Google","Cloud","Vision","V1","DataTypes"],"description":"<h4>Google::Cloud::Vision::V1</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1\">Google::Cloud::Vision::V1</a></td>\n      <td><h1 id=\"ruby-client-for-cloud-vision-api-alpha\">Ruby Client for Cloud Vision API (<a href=\"https://github.com/GoogleCloudPlatform/google-cloud-ruby#versioning\">Alpha</a>)</h1>\n\n<p><a href=\"https://cloud.google.com/vision\">Cloud Vision API</a>:\nIntegrates Google Vision features, including image labeling, face, logo, and\nlandmark detection, optical character recognition (OCR), and detection of\nexplicit content, into applications.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/annotatefileresponse\">Google::Cloud::Vision::V1::AnnotateFileResponse</a></td>\n      <td>Response to a single file annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/annotateimagerequest\">Google::Cloud::Vision::V1::AnnotateImageRequest</a></td>\n      <td>Request for performing Google Cloud Vision API tasks over a user-provided\nimage, with user-requested features.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/annotateimageresponse\">Google::Cloud::Vision::V1::AnnotateImageResponse</a></td>\n      <td>Response to an image annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/asyncannotatefilerequest\">Google::Cloud::Vision::V1::AsyncAnnotateFileRequest</a></td>\n      <td>An offline file annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/asyncannotatefileresponse\">Google::Cloud::Vision::V1::AsyncAnnotateFileResponse</a></td>\n      <td>The response for a single offline file annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/asyncbatchannotatefilesrequest\">Google::Cloud::Vision::V1::AsyncBatchAnnotateFilesRequest</a></td>\n      <td>Multiple async file annotation requests are batched into a single service\ncall.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/asyncbatchannotatefilesresponse\">Google::Cloud::Vision::V1::AsyncBatchAnnotateFilesResponse</a></td>\n      <td>Response to an async batch file annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/batchannotateimagesrequest\">Google::Cloud::Vision::V1::BatchAnnotateImagesRequest</a></td>\n      <td>Multiple image annotation requests are batched into a single service call.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/batchannotateimagesresponse\">Google::Cloud::Vision::V1::BatchAnnotateImagesResponse</a></td>\n      <td>Response to a batch image annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/block\">Google::Cloud::Vision::V1::Block</a></td>\n      <td>Logical element on the page.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/block/blocktype\">Google::Cloud::Vision::V1::Block::BlockType</a></td>\n      <td>Type of a block (text, image etc) as identified by OCR.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/boundingpoly\">Google::Cloud::Vision::V1::BoundingPoly</a></td>\n      <td>A bounding polygon for the detected image annotation.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/colorinfo\">Google::Cloud::Vision::V1::ColorInfo</a></td>\n      <td>Color information consists of RGB channels, score, and the fraction of\nthe image that the color occupies in the image.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/crophint\">Google::Cloud::Vision::V1::CropHint</a></td>\n      <td>Single crop hint that is used to generate a new crop when serving an image.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/crophintsannotation\">Google::Cloud::Vision::V1::CropHintsAnnotation</a></td>\n      <td>Set of crop hints that are used to generate new crops when serving images.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/crophintsparams\">Google::Cloud::Vision::V1::CropHintsParams</a></td>\n      <td>Parameters for crop hints annotation request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/dominantcolorsannotation\">Google::Cloud::Vision::V1::DominantColorsAnnotation</a></td>\n      <td>Set of dominant colors and their corresponding scores.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/entityannotation\">Google::Cloud::Vision::V1::EntityAnnotation</a></td>\n      <td>Set of detected entity features.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/faceannotation\">Google::Cloud::Vision::V1::FaceAnnotation</a></td>\n      <td>A face annotation object contains the results of face detection.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/faceannotation/landmark\">Google::Cloud::Vision::V1::FaceAnnotation::Landmark</a></td>\n      <td>A face-specific landmark (for example, a face feature).</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/faceannotation/landmark/type\">Google::Cloud::Vision::V1::FaceAnnotation::Landmark::Type</a></td>\n      <td>Face landmark (feature) type.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/feature\">Google::Cloud::Vision::V1::Feature</a></td>\n      <td>The type of Google Cloud Vision API detection to perform, and the maximum\nnumber of results to return for that type.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/feature/type\">Google::Cloud::Vision::V1::Feature::Type</a></td>\n      <td>Type of Google Cloud Vision API feature to be extracted.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/gcsdestination\">Google::Cloud::Vision::V1::GcsDestination</a></td>\n      <td>The Google Cloud Storage location where the output will be written to.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/gcssource\">Google::Cloud::Vision::V1::GcsSource</a></td>\n      <td>The Google Cloud Storage location where the input will be read from.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/image\">Google::Cloud::Vision::V1::Image</a></td>\n      <td>Client image to perform Google Cloud Vision API tasks over.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/imageannotationcontext\">Google::Cloud::Vision::V1::ImageAnnotationContext</a></td>\n      <td>If an image was produced from a file (e.g.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/imageannotatorclient\">Google::Cloud::Vision::V1::ImageAnnotatorClient</a></td>\n      <td>Service that performs Google Cloud Vision API detection tasks over client\nimages, such as face, landmark, logo, label, and text detection.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/imagecontext\">Google::Cloud::Vision::V1::ImageContext</a></td>\n      <td>Image context and/or feature-specific parameters.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/imageproperties\">Google::Cloud::Vision::V1::ImageProperties</a></td>\n      <td>Stores image properties, such as dominant colors.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/imagesource\">Google::Cloud::Vision::V1::ImageSource</a></td>\n      <td>External image source (Google Cloud Storage or web URL image location).</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/inputconfig\">Google::Cloud::Vision::V1::InputConfig</a></td>\n      <td>The desired input location and metadata.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/latlongrect\">Google::Cloud::Vision::V1::LatLongRect</a></td>\n      <td>Rectangle determined by min and max +LatLng+ pairs.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/likelihood\">Google::Cloud::Vision::V1::Likelihood</a></td>\n      <td>A bucketized representation of likelihood, which is intended to give clients\nhighly stable results across model upgrades.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/localizedobjectannotation\">Google::Cloud::Vision::V1::LocalizedObjectAnnotation</a></td>\n      <td>Set of detected objects with bounding boxes.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/locationinfo\">Google::Cloud::Vision::V1::LocationInfo</a></td>\n      <td>Detected entity location information.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/normalizedvertex\">Google::Cloud::Vision::V1::NormalizedVertex</a></td>\n      <td>A vertex represents a 2D point in the image.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/operationmetadata\">Google::Cloud::Vision::V1::OperationMetadata</a></td>\n      <td>Contains metadata for the BatchAnnotateImages operation.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/operationmetadata/state\">Google::Cloud::Vision::V1::OperationMetadata::State</a></td>\n      <td>Batch operation states.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/outputconfig\">Google::Cloud::Vision::V1::OutputConfig</a></td>\n      <td>The desired output location and metadata.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/page\">Google::Cloud::Vision::V1::Page</a></td>\n      <td>Detected page from OCR.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/paragraph\">Google::Cloud::Vision::V1::Paragraph</a></td>\n      <td>Structural unit of text representing a number of words in certain order.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/position\">Google::Cloud::Vision::V1::Position</a></td>\n      <td>A 3D position in the image, used primarily for Face detection landmarks.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/property\">Google::Cloud::Vision::V1::Property</a></td>\n      <td>A +Property+ consists of a user-supplied name/value pair.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/safesearchannotation\">Google::Cloud::Vision::V1::SafeSearchAnnotation</a></td>\n      <td>Set of features pertaining to the image, computed by computer vision\nmethods over safe-search verticals (for example, adult, spoof, medical,\nviolence).</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/symbol\">Google::Cloud::Vision::V1::Symbol</a></td>\n      <td>A single symbol representation.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/textannotation\">Google::Cloud::Vision::V1::TextAnnotation</a></td>\n      <td>TextAnnotation contains a structured representation of OCR extracted text.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/textannotation/detectedbreak\">Google::Cloud::Vision::V1::TextAnnotation::DetectedBreak</a></td>\n      <td>Detected start or end of a structural component.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/textannotation/detectedbreak/breaktype\">Google::Cloud::Vision::V1::TextAnnotation::DetectedBreak::BreakType</a></td>\n      <td>Enum to denote the type of break found.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/textannotation/detectedlanguage\">Google::Cloud::Vision::V1::TextAnnotation::DetectedLanguage</a></td>\n      <td>Detected language for a structural component.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/textannotation/textproperty\">Google::Cloud::Vision::V1::TextAnnotation::TextProperty</a></td>\n      <td>Additional information detected on the structural component.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/vertex\">Google::Cloud::Vision::V1::Vertex</a></td>\n      <td>A vertex represents a 2D point in the image.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetection\">Google::Cloud::Vision::V1::WebDetection</a></td>\n      <td>Relevant information for the image from the Internet.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetection/webentity\">Google::Cloud::Vision::V1::WebDetection::WebEntity</a></td>\n      <td>Entity deduced from similar images on the Internet.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetection/webimage\">Google::Cloud::Vision::V1::WebDetection::WebImage</a></td>\n      <td>Metadata for online images.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetection/weblabel\">Google::Cloud::Vision::V1::WebDetection::WebLabel</a></td>\n      <td>Label to provide extra metadata for the web detection.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetection/webpage\">Google::Cloud::Vision::V1::WebDetection::WebPage</a></td>\n      <td>Metadata for web pages.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/webdetectionparams\">Google::Cloud::Vision::V1::WebDetectionParams</a></td>\n      <td>Parameters for web detection request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/vision/v1/word\">Google::Cloud::Vision::V1::Word</a></td>\n      <td>A word representation.</td>\n    </tr>\n\n  </tbody>\n</table>\n<h4>Google::Protobuf</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/any\">Google::Protobuf::Any</a></td>\n      <td>+Any+ contains an arbitrary serialized protocol buffer message along with a\nURL that describes the type of the serialized message.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/boolvalue\">Google::Protobuf::BoolValue</a></td>\n      <td>Wrapper message for +bool+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/bytesvalue\">Google::Protobuf::BytesValue</a></td>\n      <td>Wrapper message for +bytes+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/doublevalue\">Google::Protobuf::DoubleValue</a></td>\n      <td>Wrapper message for +double+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/floatvalue\">Google::Protobuf::FloatValue</a></td>\n      <td>Wrapper message for +float+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/int32value\">Google::Protobuf::Int32Value</a></td>\n      <td>Wrapper message for +int32+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/int64value\">Google::Protobuf::Int64Value</a></td>\n      <td>Wrapper message for +int64+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/stringvalue\">Google::Protobuf::StringValue</a></td>\n      <td>Wrapper message for +string+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/uint32value\">Google::Protobuf::UInt32Value</a></td>\n      <td>Wrapper message for +uint32+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/uint64value\">Google::Protobuf::UInt64Value</a></td>\n      <td>Wrapper message for +uint64+.</td>\n    </tr>\n\n  </tbody>\n</table>\n<h4>Google::Rpc</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/rpc/status\">Google::Rpc::Status</a></td>\n      <td>The +Status+ type defines a logical error model that is suitable for different\nprogramming environments, including REST APIs and RPC APIs.</td>\n    </tr>\n\n  </tbody>\n</table>\n<h4>Google::Type</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/type/color\">Google::Type::Color</a></td>\n      <td>Represents a color in the RGBA color space.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/type/latlng\">Google::Type::LatLng</a></td>\n      <td>An object representing a latitude/longitude pair.</td>\n    </tr>\n\n  </tbody>\n</table>\n\n","source":"","resources":[],"examples":[],"methods":[]}