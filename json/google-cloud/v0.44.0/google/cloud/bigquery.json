{"id":"google/cloud/bigquery","name":"Bigquery","title":["Google","Cloud","Bigquery"],"description":"<h1 id=\"google-cloud-bigquery\">Google Cloud BigQuery</h1>\n\n<p>Google BigQuery enables super-fast, SQL-like queries against massive\ndatasets, using the processing power of Google’s infrastructure. To learn\nmore, read <a href=\"https://cloud.google.com/bigquery/what-is-bigquery\">What is\nBigQuery?</a>.</p>\n\n<p>The goal of google-cloud is to provide an API that is comfortable to\nRubyists. Authentication is handled by Google::Cloud#bigquery. You can\nprovide the project and credential information to connect to the BigQuery\nservice, or if you are running on Google Compute Engine this configuration\nis taken care of for you. You can read more about the options for\nconnecting in the <a href=\"https://googlecloudplatform.github.io/google-cloud-ruby/#/docs/guides/authentication\">Authentication\nGuide</a>.</p>\n\n<p>To help you get started quickly, the first few examples below use a public\ndataset provided by Google. As soon as you have <a href=\"https://cloud.google.com/bigquery/sign-up\">signed\nup</a> to use BigQuery, and\nprovided that you stay in the free tier for queries, you should be able to\nrun these first examples without the need to set up billing or to load\ndata (although we’ll show you how to do that too.)</p>\n\n<h2 id=\"listing-datasets-and-tables\">Listing Datasets and Tables</h2>\n\n<p>A BigQuery project contains datasets, which in turn contain tables.\nAssuming that you have not yet created datasets or tables in your own\nproject, let’s connect to Google’s <code>publicdata</code> project, and see what we\nfind.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new project: \"publicdata\"\n\nbigquery.datasets.count #=&gt; 1\nbigquery.datasets.first.dataset_id #=&gt; \"samples\"\n\ndataset = bigquery.datasets.first\ntables = dataset.tables\n\ntables.count #=&gt; 7\ntables.map &amp;:table_id #=&gt; [..., \"shakespeare\", \"trigrams\", \"wikipedia\"]\n</code></pre>\n\n<p>In addition to listing all datasets and tables in the project, you can\nalso retrieve individual datasets and tables by ID. Let’s look at the\nstructure of the <code>shakespeare</code> table, which contains an entry for every\nword in every play written by Shakespeare.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new project: \"publicdata\"\n\ndataset = bigquery.dataset \"samples\"\ntable = dataset.table \"shakespeare\"\n\ntable.headers #=&gt; [:word, :word_count, :corpus, :corpus_date]\ntable.rows_count #=&gt; 164656\n</code></pre>\n\n<p>Now that you know the column names for the Shakespeare table, let’s\nwrite and run a few queries against it.</p>\n\n<h2 id=\"running-queries\">Running queries</h2>\n\n<p>BigQuery supports two SQL dialects: <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/\">standard\nSQL</a>\nand the older <a href=\"https://cloud.google.com/bigquery/docs/reference/legacy-sql\">legacy SQl (BigQuery\nSQL)</a>,\nas discussed in the guide <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql\">Migrating from legacy\nSQL</a>.</p>\n\n<p>In addition, BigQuery offers both synchronous and asynchronous methods, as\nexplained in <a href=\"https://cloud.google.com/bigquery/querying-data\">Querying\nData</a>.</p>\n\n<h3 id=\"standard-sql\">Standard SQL</h3>\n\n<p>Standard SQL is the preferred SQL dialect for querying data stored in\nBigQuery. It is compliant with the SQL 2011 standard, and has extensions\nthat support querying nested and repeated data. This is the default\nsyntax. It has several advantages over legacy SQL, including:</p>\n\n<ul>\n  <li>Composability using <code>WITH</code> clauses and SQL functions</li>\n  <li>Subqueries in the <code>SELECT</code> list and <code>WHERE</code> clause</li>\n  <li>Correlated subqueries</li>\n  <li><code>ARRAY</code> and <code>STRUCT</code> data types</li>\n  <li>Inserts, updates, and deletes</li>\n  <li><code>COUNT(DISTINCT &lt;expr&gt;)</code> is exact and scalable, providing the accuracy\nof <code>EXACT_COUNT_DISTINCT</code> without its limitations</li>\n  <li>Automatic predicate push-down through <code>JOIN</code>s</li>\n  <li>Complex <code>JOIN</code> predicates, including arbitrary expressions</li>\n</ul>\n\n<p>For examples that demonstrate some of these features, see <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql#standard_sql_highlights\">Standard SQL\nhighlights</a>.</p>\n\n<p>As shown in this example, standard SQL is the library default:</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\nsql = \"SELECT word, SUM(word_count) AS word_count \" \\\n      \"FROM `bigquery-public-data.samples.shakespeare`\" \\\n      \"WHERE word IN ('me', 'I', 'you') GROUP BY word\"\ndata = bigquery.query sql\n</code></pre>\n\n<p>Notice that in standard SQL, a fully-qualified table name uses the\nfollowing format: <code>`my-dashed-project.dataset1.tableName`</code>.</p>\n\n<h3 id=\"legacy-sql-formerly-bigquery-sql\">Legacy SQL (formerly BigQuery SQL)</h3>\n\n<p>Before version 2.0, BigQuery executed queries using a non-standard SQL\ndialect known as BigQuery SQL. This variant is optional, and can be\nenabled by passing the flag <code>legacy_sql: true</code> with your query. (If you\nget an SQL syntax error with a query that may be written in legacy SQL,\nbe sure that you are passing this option.)</p>\n\n<p>To use legacy SQL, pass the option <code>legacy_sql: true</code> with your query:</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\nsql = \"SELECT TOP(word, 50) as word, COUNT(*) as count \" \\\n      \"FROM [publicdata:samples.shakespeare]\"\ndata = bigquery.query sql, legacy_sql: true\n</code></pre>\n\n<p>Notice that in legacy SQL, a fully-qualified table name uses brackets\ninstead of back-ticks, and a semi-colon instead of a dot to separate the\nproject and the dataset: <code>[my-dashed-project:dataset1.tableName]</code>.</p>\n\n<h4 id=\"query-parameters\">Query parameters</h4>\n\n<p>With standard SQL, you can use positional or named query parameters. This\nexample shows the use of named parameters:</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\nsql = \"SELECT word, SUM(word_count) AS word_count \" \\\n      \"FROM `bigquery-public-data.samples.shakespeare`\" \\\n      \"WHERE word IN UNNEST(@words) GROUP BY word\"\ndata = bigquery.query sql, params: { words: ['me', 'I', 'you'] }\n</code></pre>\n\n<p>As demonstrated above, passing the <code>params</code> option will automatically set\n<code>standard_sql</code> to <code>true</code>.</p>\n\n<h4 id=\"data-types\">Data types</h4>\n\n<p>BigQuery standard SQL supports simple data types such as integers, as well\nas more complex types such as <code>ARRAY</code> and <code>STRUCT</code>.</p>\n\n<p>The BigQuery data types are converted to and from Ruby types as follows:</p>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>BigQuery</th>\n      <th>Ruby</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code>BOOL</code></td>\n      <td><code>true</code>/<code>false</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>INT64</code></td>\n      <td><code>Integer</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>FLOAT64</code></td>\n      <td><code>Float</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>STRING</code></td>\n      <td><code>STRING</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>DATETIME</code></td>\n      <td><code>DateTime</code></td>\n      <td><code>DATETIME</code> does not support time zone.</td>\n    </tr>\n    <tr>\n      <td><code>DATE</code></td>\n      <td><code>Date</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>TIMESTAMP</code></td>\n      <td><code>Time</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>TIME</code></td>\n      <td><code>Google::Cloud::BigQuery::Time</code></td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>BYTES</code></td>\n      <td><code>File</code>, <code>IO</code>, <code>StringIO</code>, or similar</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><code>ARRAY</code></td>\n      <td><code>Array</code></td>\n      <td>Nested arrays and <code>nil</code> values are not supported.</td>\n    </tr>\n    <tr>\n      <td><code>STRUCT</code></td>\n      <td><code>Hash</code></td>\n      <td>Hash keys may be strings or symbols.</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>See <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types\">Data Types</a>\nfor an overview of each BigQuery data type, including allowed values.</p>\n\n<h3 id=\"synchronous-queries\">Synchronous queries</h3>\n\n<p>Let’s start with the simpler synchronous approach. Notice that this time\nyou are connecting using your own default project. It is necessary to have\nwrite access to the project for running a query, since queries need to\ncreate tables to hold results.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\nsql = \"SELECT APPROX_TOP_COUNT(corpus, 10) as title, \" \\\n      \"COUNT(*) as unique_words \" \\\n      \"FROM publicdata.samples.shakespeare\"\ndata = bigquery.query sql\n\ndata.next? #=&gt; false\ndata.first #=&gt; {:title=&gt;[{:value=&gt;\"hamlet\", :count=&gt;5318}, ...}\n</code></pre>\n\n<p>The <code>APPROX_TOP_COUNT</code> function shown above is just one of a variety of\nfunctions offered by BigQuery. See the <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators\">Query Reference (standard\nSQL)</a>\nfor a full listing.</p>\n\n<h3 id=\"asynchronous-queries\">Asynchronous queries</h3>\n\n<p>It is usually best not to block for most BigQuery operations, including\nquerying as well as importing, exporting, and copying data. Therefore, the\nBigQuery API provides facilities for managing longer-running jobs. With\nthe asynchronous approach to running a query, an instance of\n<a data-custom-type=\"google/cloud/bigquery/queryjob\">Google::Cloud::Bigquery::QueryJob</a> is returned, rather than an instance\nof <a data-custom-type=\"google/cloud/bigquery/data\">Google::Cloud::Bigquery::Data</a>.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\nsql = \"SELECT APPROX_TOP_COUNT(corpus, 10) as title, \" \\\n      \"COUNT(*) as unique_words \" \\\n      \"FROM publicdata.samples.shakespeare\"\njob = bigquery.query_job sql\n\njob.wait_until_done!\nif !job.failed?\n  job.data.first\n  #=&gt; {:title=&gt;[{:value=&gt;\"hamlet\", :count=&gt;5318}, ...}\nend\n</code></pre>\n\n<p>Once you have determined that the job is done and has not failed, you can\nobtain an instance of <a data-custom-type=\"google/cloud/bigquery/data\">Google::Cloud::Bigquery::Data</a> by calling <code>data</code> on\nthe job instance. The query results for both of the above examples are\nstored in temporary tables with a lifetime of about 24 hours. See the\nfinal example below for a demonstration of how to store query results in a\npermanent table.</p>\n\n<h2 id=\"creating-datasets-and-tables\">Creating Datasets and Tables</h2>\n\n<p>The first thing you need to do in a new BigQuery project is to create a\n<a data-custom-type=\"google/cloud/bigquery/dataset\">Google::Cloud::Bigquery::Dataset</a>. Datasets hold tables and control\naccess to them.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\n\ndataset = bigquery.create_dataset \"my_dataset\"\n</code></pre>\n\n<p>Now that you have a dataset, you can use it to create a table. Every table\nis defined by a schema that may contain nested and repeated fields. The\nexample below shows a schema with a repeated record field named\n<code>cities_lived</code>. (For more information about nested and repeated fields,\nsee <a href=\"https://cloud.google.com/bigquery/preparing-data-for-loading\">Preparing Data for\nLoading</a>.)</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\"\n\ntable = dataset.create_table \"people\" do |schema|\n  schema.string \"first_name\", mode: :required\n  schema.record \"cities_lived\", mode: :repeated do |nested_schema|\n    nested_schema.string \"place\", mode: :required\n    nested_schema.integer \"number_of_years\", mode: :required\n  end\nend\n</code></pre>\n\n<p>Because of the repeated field in this schema, we cannot use the CSV format\nto load data into the table.</p>\n\n<h2 id=\"loading-records\">Loading records</h2>\n\n<p>To follow along with these examples, you will need to set up billing on\nthe <a href=\"https://console.developers.google.com\">Google Developers Console</a>.</p>\n\n<p>In addition to CSV, data can be imported from files that are formatted as\n<a href=\"http://jsonlines.org/\">Newline-delimited JSON</a> or\n<a href=\"http://avro.apache.org/\">Avro</a>, or from a Google Cloud Datastore backup.\nIt can also be “streamed” into BigQuery.</p>\n\n<h3 id=\"streaming-records\">Streaming records</h3>\n\n<p>For situations in which you want new data to be available for querying as\nsoon as possible, inserting individual records directly from your Ruby\napplication is a great approach.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\"\ntable = dataset.table \"people\"\n\nrows = [\n    {\n        \"first_name\" =&gt; \"Anna\",\n        \"cities_lived\" =&gt; [\n            {\n                \"place\" =&gt; \"Stockholm\",\n                \"number_of_years\" =&gt; 2\n            }\n        ]\n    },\n    {\n        \"first_name\" =&gt; \"Bob\",\n        \"cities_lived\" =&gt; [\n            {\n                \"place\" =&gt; \"Seattle\",\n                \"number_of_years\" =&gt; 5\n            },\n            {\n                \"place\" =&gt; \"Austin\",\n                \"number_of_years\" =&gt; 6\n            }\n        ]\n    }\n]\ntable.insert rows\n</code></pre>\n\n<p>To avoid making RPCs (network requests) to retrieve the dataset and table\nresources when streaming records, pass the <code>skip_lookup</code> option. This\ncreates local objects without verifying that the resources exist on the\nBigQuery service.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\", skip_lookup: true\ntable = dataset.table \"people\", skip_lookup: true\n\nrows = [\n    {\n        \"first_name\" =&gt; \"Anna\",\n        \"cities_lived\" =&gt; [\n            {\n                \"place\" =&gt; \"Stockholm\",\n                \"number_of_years\" =&gt; 2\n            }\n        ]\n    },\n    {\n        \"first_name\" =&gt; \"Bob\",\n        \"cities_lived\" =&gt; [\n            {\n                \"place\" =&gt; \"Seattle\",\n                \"number_of_years\" =&gt; 5\n            },\n            {\n                \"place\" =&gt; \"Austin\",\n                \"number_of_years\" =&gt; 6\n            }\n        ]\n    }\n]\ntable.insert rows\n</code></pre>\n\n<p>There are some trade-offs involved with streaming, so be sure to read the\ndiscussion of data consistency in <a href=\"https://cloud.google.com/bigquery/streaming-data-into-bigquery\">Streaming Data Into\nBigQuery</a>.</p>\n\n<h3 id=\"uploading-a-file\">Uploading a file</h3>\n\n<p>To follow along with this example, please download the\n<a href=\"http://www.ssa.gov/OACT/babynames/names.zip\">names.zip</a> archive from the\nU.S. Social Security Administration. Inside the archive you will find over\n100 files containing baby name records since the year 1880.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\"\ntable = dataset.create_table \"baby_names\" do |schema|\n  schema.string \"name\", mode: :required\n  schema.string \"gender\", mode: :required\n  schema.integer \"count\", mode: :required\nend\n\nfile = File.open \"names/yob2014.txt\"\ntable.load file, format: \"csv\"\n</code></pre>\n\n<p>Because the names data, although formatted as CSV, is distributed in files\nwith a <code>.txt</code> extension, this example explicitly passes the <code>format</code>\noption in order to demonstrate how to handle such situations. Because CSV\nis the default format for load operations, the option is not actually\nnecessary. For JSON saved with a <code>.txt</code> extension, however, it would be.</p>\n\n<h2 id=\"exporting-query-results-to-google-cloud-storage\">Exporting query results to Google Cloud Storage</h2>\n\n<p>The example below shows how to pass the <code>table</code> option with a query in\norder to store results in a permanent table. It also shows how to export\nthe result data to a Google Cloud Storage file. In order to follow along,\nyou will need to enable the Google Cloud Storage API in addition to\nsetting up billing.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\"\nsource_table = dataset.table \"baby_names\"\nresult_table = dataset.create_table \"baby_names_results\"\n\nsql = \"SELECT name, count \" \\\n      \"FROM baby_names \" \\\n      \"WHERE gender = 'M' \" \\\n      \"ORDER BY count ASC LIMIT 5\"\nquery_job = dataset.query_job sql, table: result_table\n\nquery_job.wait_until_done!\n\nif !query_job.failed?\n  require \"google/cloud/storage\"\n\n  storage = Google::Cloud::Storage.new\n  bucket_id = \"bigquery-exports-#{SecureRandom.uuid}\"\n  bucket = storage.create_bucket bucket_id\n  extract_url = \"gs://#{bucket.id}/baby-names.csv\"\n\n  result_table.extract extract_url\n\n  # Download to local filesystem\n  bucket.files.first.download \"baby-names.csv\"\nend\n</code></pre>\n\n<p>If a table you wish to export contains a large amount of data, you can\npass a wildcard URI to export to multiple files (for sharding), or an\narray of URIs (for partitioning), or both. See <a href=\"https://cloud.google.com/bigquery/docs/exporting-data\">Exporting\nData</a>\nfor details.</p>\n\n<h2 id=\"configuring-retries-and-timeout\">Configuring retries and timeout</h2>\n\n<p>You can configure how many times API requests may be automatically\nretried. When an API request fails, the response will be inspected to see\nif the request meets criteria indicating that it may succeed on retry,\nsuch as <code>500</code> and <code>503</code> status codes or a specific internal error code\nsuch as <code>rateLimitExceeded</code>. If it meets the criteria, the request will be\nretried after a delay. If another error occurs, the delay will be\nincreased before a subsequent attempt, until the <code>retries</code> limit is\nreached.</p>\n\n<p>You can also set the request <code>timeout</code> value in seconds.</p>\n\n<pre><code class=\"language-ruby\">require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new retries: 10, timeout: 120\n</code></pre>\n\n<p>See the <a href=\"https://cloud.google.com/bigquery/troubleshooting-errors#errortable\">BigQuery error\ntable</a>\nfor a list of error conditions.</p>","source":"google-cloud-bigquery/lib/google/cloud/bigquery.rb#L489","resources":[],"examples":[],"methods":[{"id":"new-class","type":"class","name":"new","title":["Google","Cloud","Bigquery.new"],"description":"<p>Creates a new <code>Project</code> instance connected to the BigQuery service.\nEach call creates a new connection.</p>\n\n<p>For more information on connecting to Google Cloud see the\n<a href=\"https://googlecloudplatform.github.io/google-cloud-ruby/#/docs/guides/authentication\">Authentication\nGuide</a>.</p>","source":"google-cloud-bigquery/lib/google/cloud/bigquery.rb#L526","resources":[],"examples":[{"caption":"","code":"require \"google/cloud/bigquery\"\n\nbigquery = Google::Cloud::Bigquery.new\ndataset = bigquery.dataset \"my_dataset\"\ntable = dataset.table \"my_table\""}],"params":[{"name":"project_id","types":["String"],"description":"Identifier for a BigQuery project. If not\npresent, the default project for the credentials is used.","optional":true,"default":"nil","nullable":true},{"name":"credentials","types":["String","Hash","Google::Auth::Credentials"],"description":"The path to\nthe keyfile as a String, the contents of the keyfile as a Hash, or a\nGoogle::Auth::Credentials object. (See <a data-custom-type=\"google/cloud/bigquery/credentials\">Bigquery::Credentials</a>)","optional":true,"default":"nil","nullable":true},{"name":"scope","types":["String","Array<String>"],"description":"The OAuth 2.0 scopes controlling\nthe set of resources and operations that the connection can access.\nSee # <a href=\"https://developers.google.com/identity/protocols/OAuth2\">Using OAuth 2.0 to Access Google #\nAPIs</a>.</p>\n\n<p>The default scope is:</p>\n\n<ul>\n  <li><code>https://www.googleapis.com/auth/bigquery</code></li>\n</ul>","optional":true,"default":"nil","nullable":true},{"name":"retries","types":["Integer"],"description":"Number of times to retry requests on server\nerror. The default value is <code>5</code>. Optional.","optional":true,"default":"nil","nullable":true},{"name":"timeout","types":["Integer"],"description":"Default timeout to use in requests. Optional.","optional":true,"default":"nil","nullable":true},{"name":"project","types":["String"],"description":"Alias for the <code>project_id</code> argument. Deprecated.","optional":true,"default":"nil","nullable":true},{"name":"keyfile","types":["String"],"description":"Alias for the <code>credentials</code> argument.\nDeprecated.","optional":true,"default":"nil","nullable":true}],"exceptions":[],"returns":[{"types":["<a data-custom-type=\"google/cloud/bigquery/project\">Google::Cloud::Bigquery::Project</a>"],"description":""}]}]}