{"id":"google/cloud/speech/v1/datatypes","name":"DataTypes","title":["Google","Cloud","Speech","V1","DataTypes"],"description":"<h4>Google::Cloud::Speech::V1</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1\">Google::Cloud::Speech::V1</a></td>\n      <td><h1 id=\"ruby-client-for-google-cloud-speech-api-alpha\">Ruby Client for Google Cloud Speech API (<a href=\"https://github.com/GoogleCloudPlatform/google-cloud-ruby#versioning\">Alpha</a>)</h1>\n\n<p><a href=\"https://cloud.google.com/speech\">Google Cloud Speech API</a>:\nGoogle Cloud Speech API.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/longrunningrecognizemetadata\">Google::Cloud::Speech::V1::LongRunningRecognizeMetadata</a></td>\n      <td>Describes the progress of a long-running +LongRunningRecognize+ call.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/longrunningrecognizerequest\">Google::Cloud::Speech::V1::LongRunningRecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +LongRunningRecognize+\nmethod.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/longrunningrecognizeresponse\">Google::Cloud::Speech::V1::LongRunningRecognizeResponse</a></td>\n      <td>The only message returned to the client by the +LongRunningRecognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/recognitionaudio\">Google::Cloud::Speech::V1::RecognitionAudio</a></td>\n      <td>Contains audio data in the encoding specified in the +RecognitionConfig+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/recognitionconfig\">Google::Cloud::Speech::V1::RecognitionConfig</a></td>\n      <td>Provides information to the recognizer that specifies how to process the\nrequest.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/recognitionconfig/audioencoding\">Google::Cloud::Speech::V1::RecognitionConfig::AudioEncoding</a></td>\n      <td>Audio encoding of the data sent in the audio message.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/recognizerequest\">Google::Cloud::Speech::V1::RecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +Recognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/recognizeresponse\">Google::Cloud::Speech::V1::RecognizeResponse</a></td>\n      <td>The only message returned to the client by the +Recognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/speechclient\">Google::Cloud::Speech::V1::SpeechClient</a></td>\n      <td>Service that implements Google Cloud Speech API.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/speechcontext\">Google::Cloud::Speech::V1::SpeechContext</a></td>\n      <td>Provides “hints” to the speech recognizer to favor specific words and phrases\nin the results.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/speechrecognitionalternative\">Google::Cloud::Speech::V1::SpeechRecognitionAlternative</a></td>\n      <td>Alternative hypotheses (a.k.a.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/speechrecognitionresult\">Google::Cloud::Speech::V1::SpeechRecognitionResult</a></td>\n      <td>A speech recognition result corresponding to a portion of the audio.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/stream\">Google::Cloud::Speech::V1::Stream</a></td>\n      <td><h1 id=\"stream\">Stream</h1>\n\n<p>A resource that represents the streaming requests and responses.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/streamingrecognitionconfig\">Google::Cloud::Speech::V1::StreamingRecognitionConfig</a></td>\n      <td>Provides information to the recognizer that specifies how to process the\nrequest.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/streamingrecognitionresult\">Google::Cloud::Speech::V1::StreamingRecognitionResult</a></td>\n      <td>A streaming speech recognition result corresponding to a portion of the audio\nthat is currently being processed.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/streamingrecognizerequest\">Google::Cloud::Speech::V1::StreamingRecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +StreamingRecognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/streamingrecognizeresponse\">Google::Cloud::Speech::V1::StreamingRecognizeResponse</a></td>\n      <td>+StreamingRecognizeResponse+ is the only message returned to the client by\n+StreamingRecognize+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/streamingrecognizeresponse/speecheventtype\">Google::Cloud::Speech::V1::StreamingRecognizeResponse::SpeechEventType</a></td>\n      <td>Indicates the type of speech event.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1/wordinfo\">Google::Cloud::Speech::V1::WordInfo</a></td>\n      <td>Word-specific information for recognized words.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1\">Google::Cloud::Speech::V1p1beta1</a></td>\n      <td><h1 id=\"ruby-client-for-cloud-speech-api-alpha\">Ruby Client for Cloud Speech API (<a href=\"https://github.com/GoogleCloudPlatform/google-cloud-ruby#versioning\">Alpha</a>)</h1>\n\n<p><a href=\"https://cloud.google.com/speech\">Cloud Speech API</a>:\nConverts audio to text by applying powerful neural network models.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/longrunningrecognizemetadata\">Google::Cloud::Speech::V1p1beta1::LongRunningRecognizeMetadata</a></td>\n      <td>Describes the progress of a long-running +LongRunningRecognize+ call.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/longrunningrecognizerequest\">Google::Cloud::Speech::V1p1beta1::LongRunningRecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +LongRunningRecognize+\nmethod.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/longrunningrecognizeresponse\">Google::Cloud::Speech::V1p1beta1::LongRunningRecognizeResponse</a></td>\n      <td>The only message returned to the client by the +LongRunningRecognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionaudio\">Google::Cloud::Speech::V1p1beta1::RecognitionAudio</a></td>\n      <td>Contains audio data in the encoding specified in the +RecognitionConfig+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionconfig\">Google::Cloud::Speech::V1p1beta1::RecognitionConfig</a></td>\n      <td>Provides information to the recognizer that specifies how to process the\nrequest.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionconfig/audioencoding\">Google::Cloud::Speech::V1p1beta1::RecognitionConfig::AudioEncoding</a></td>\n      <td>The encoding of the audio data sent in the request.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionmetadata\">Google::Cloud::Speech::V1p1beta1::RecognitionMetadata</a></td>\n      <td>Description of audio data to be recognized.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionmetadata/interactiontype\">Google::Cloud::Speech::V1p1beta1::RecognitionMetadata::InteractionType</a></td>\n      <td>Use case categories that the audio recognition request can be described\nby.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionmetadata/microphonedistance\">Google::Cloud::Speech::V1p1beta1::RecognitionMetadata::MicrophoneDistance</a></td>\n      <td>Enumerates the types of capture settings describing an audio file.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionmetadata/originalmediatype\">Google::Cloud::Speech::V1p1beta1::RecognitionMetadata::OriginalMediaType</a></td>\n      <td>The original media the speech was recorded on.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognitionmetadata/recordingdevicetype\">Google::Cloud::Speech::V1p1beta1::RecognitionMetadata::RecordingDeviceType</a></td>\n      <td>The type of device the speech was recorded with.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognizerequest\">Google::Cloud::Speech::V1p1beta1::RecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +Recognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/recognizeresponse\">Google::Cloud::Speech::V1p1beta1::RecognizeResponse</a></td>\n      <td>The only message returned to the client by the +Recognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/speechclient\">Google::Cloud::Speech::V1p1beta1::SpeechClient</a></td>\n      <td>Service that implements Google Cloud Speech API.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/speechcontext\">Google::Cloud::Speech::V1p1beta1::SpeechContext</a></td>\n      <td>Provides “hints” to the speech recognizer to favor specific words and phrases\nin the results.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/speechrecognitionalternative\">Google::Cloud::Speech::V1p1beta1::SpeechRecognitionAlternative</a></td>\n      <td>Alternative hypotheses (a.k.a.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/speechrecognitionresult\">Google::Cloud::Speech::V1p1beta1::SpeechRecognitionResult</a></td>\n      <td>A speech recognition result corresponding to a portion of the audio.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/stream\">Google::Cloud::Speech::V1p1beta1::Stream</a></td>\n      <td><h1 id=\"stream\">Stream</h1>\n\n<p>A resource that represents the streaming requests and responses.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/streamingrecognitionconfig\">Google::Cloud::Speech::V1p1beta1::StreamingRecognitionConfig</a></td>\n      <td>Provides information to the recognizer that specifies how to process the\nrequest.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/streamingrecognitionresult\">Google::Cloud::Speech::V1p1beta1::StreamingRecognitionResult</a></td>\n      <td>A streaming speech recognition result corresponding to a portion of the audio\nthat is currently being processed.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/streamingrecognizerequest\">Google::Cloud::Speech::V1p1beta1::StreamingRecognizeRequest</a></td>\n      <td>The top-level message sent by the client for the +StreamingRecognize+ method.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/streamingrecognizeresponse\">Google::Cloud::Speech::V1p1beta1::StreamingRecognizeResponse</a></td>\n      <td>+StreamingRecognizeResponse+ is the only message returned to the client by\n+StreamingRecognize+.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/streamingrecognizeresponse/speecheventtype\">Google::Cloud::Speech::V1p1beta1::StreamingRecognizeResponse::SpeechEventType</a></td>\n      <td>Indicates the type of speech event.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/cloud/speech/v1p1beta1/wordinfo\">Google::Cloud::Speech::V1p1beta1::WordInfo</a></td>\n      <td>Word-specific information for recognized words.</td>\n    </tr>\n\n  </tbody>\n</table>\n<h4>Google::Protobuf</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/any\">Google::Protobuf::Any</a></td>\n      <td>+Any+ contains an arbitrary serialized protocol buffer message along with a\nURL that describes the type of the serialized message.</td>\n    </tr>\n\n    <tr>\n      <td><a data-custom-type=\"google/protobuf/duration\">Google::Protobuf::Duration</a></td>\n      <td>A Duration represents a signed, fixed-length span of time represented\nas a count of seconds and fractions of seconds at nanosecond\nresolution.</td>\n    </tr>\n\n  </tbody>\n</table>\n<h4>Google::Rpc</h4>\n\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Class</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n\n    <tr>\n      <td><a data-custom-type=\"google/rpc/status\">Google::Rpc::Status</a></td>\n      <td>The +Status+ type defines a logical error model that is suitable for different\nprogramming environments, including REST APIs and RPC APIs.</td>\n    </tr>\n\n  </tbody>\n</table>\n\n","source":"","resources":[],"examples":[],"methods":[]}